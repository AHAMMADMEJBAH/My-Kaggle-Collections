{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.010862,
     "end_time": "2020-09-04T21:35:10.977024",
     "exception": false,
     "start_time": "2020-09-04T21:35:10.966162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Selecting Best Models Using Exhaustive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.009523,
     "end_time": "2020-09-04T21:35:10.995626",
     "exception": false,
     "start_time": "2020-09-04T21:35:10.986103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem : \n",
    "You want to select the best model by searching over a range of hyperparameters.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00774,
     "end_time": "2020-09-04T21:35:11.011363",
     "exception": false,
     "start_time": "2020-09-04T21:35:11.003623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution: \n",
    "Use scikit-learn’s GridSearchCV:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-04T21:35:11.030640Z",
     "iopub.status.busy": "2020-09-04T21:35:11.030013Z",
     "iopub.status.idle": "2020-09-04T21:35:14.547764Z",
     "shell.execute_reply": "2020-09-04T21:35:14.548540Z"
    },
    "papermill": {
     "duration": 3.52993,
     "end_time": "2020-09-04T21:35:14.548810",
     "exception": false,
     "start_time": "2020-09-04T21:35:11.018880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 7.742636826811269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "# Create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "# Create range of candidate regularization hyperparameter values\n",
    "C = np.logspace(0, 4, 10)\n",
    "# Create dictionary hyperparameter candidates\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "np.logspace(0, 4, 10)\n",
    "\n",
    "\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.01278,
     "end_time": "2020-09-04T21:35:14.574789",
     "exception": false,
     "start_time": "2020-09-04T21:35:14.562009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "\n",
    "GridSearchCV is a brute-force approach to model selection using cross-validation.\n",
    "Specifically, a user defines sets of possible values for one or multiple hyperparameters,\n",
    "and then GridSearchCV trains a model using every value and/or combination of values.\n",
    "The model with the best performance score is selected as the best model. For\n",
    "example, in our solution we used logistic regression as our learning algorithm, containing\n",
    "two hyperparameters: C and the regularization penalty. Don’t worry if you\n",
    "don’t know what C and regularization mean; we cover them in the next few chapters.\n",
    "Just realize that C and the regularization penalty can take a range of values, which\n",
    "have to be specified prior to training. For C, we define 10 possible values:"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012988,
     "end_time": "2020-09-04T21:35:14.601099",
     "exception": false,
     "start_time": "2020-09-04T21:35:14.588111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Selecting Best Models Using Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012297,
     "end_time": "2020-09-04T21:35:14.626128",
     "exception": false,
     "start_time": "2020-09-04T21:35:14.613831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem\n",
    "You want a computationally cheaper method than exhaustive search to select the best\n",
    "model.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011963,
     "end_time": "2020-09-04T21:35:14.651103",
     "exception": false,
     "start_time": "2020-09-04T21:35:14.639140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution\n",
    "Use scikit-learn’s RandomizedSearchCV:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-04T21:35:14.715797Z",
     "iopub.status.busy": "2020-09-04T21:35:14.715103Z",
     "iopub.status.idle": "2020-09-04T21:35:21.533007Z",
     "shell.execute_reply": "2020-09-04T21:35:21.532488Z"
    },
    "papermill": {
     "duration": 6.867781,
     "end_time": "2020-09-04T21:35:21.533117",
     "exception": false,
     "start_time": "2020-09-04T21:35:14.665336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 3.730229437354635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "# Create distribution of candidate regularization hyperparameter values\n",
    "C = uniform(loc=0, scale=4)\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "# Create randomized search\n",
    "randomizedsearch = RandomizedSearchCV(\n",
    "logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
    "n_jobs=-1)\n",
    "# Fit randomized search\n",
    "best_model = randomizedsearch.fit(features, target)\n",
    "\n",
    "# Define a uniform distribution between 0 and 4, sample 10 values\n",
    "uniform(loc=0, scale=4).rvs(10)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007241,
     "end_time": "2020-09-04T21:35:21.548709",
     "exception": false,
     "start_time": "2020-09-04T21:35:21.541468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "In Recipe 1, we used GridSearchCV on a user-defined set of hyperparameter values\n",
    "to search for the best model according to a score function. A more efficient method\n",
    "than GridSearchCV’s brute-force search is to search over a specific number of random\n",
    "combinations of hyperparameter values from user-supplied distributions (e.g., normal,\n",
    "uniform). scikit-learn implements this randomized search technique with Ran\n",
    "domizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007545,
     "end_time": "2020-09-04T21:35:21.564141",
     "exception": false,
     "start_time": "2020-09-04T21:35:21.556596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Selecting Best Models from Multiple Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007635,
     "end_time": "2020-09-04T21:35:21.579557",
     "exception": false,
     "start_time": "2020-09-04T21:35:21.571922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem\n",
    "You want to select the best model by searching over a range of learning algorithms\n",
    "and their respective hyperparameters.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008105,
     "end_time": "2020-09-04T21:35:21.595832",
     "exception": false,
     "start_time": "2020-09-04T21:35:21.587727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution: \n",
    "Create a dictionary of candidate learning algorithms and their hyperparameters:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-04T21:35:21.624606Z",
     "iopub.status.busy": "2020-09-04T21:35:21.621171Z",
     "iopub.status.idle": "2020-09-04T21:35:55.762160Z",
     "shell.execute_reply": "2020-09-04T21:35:55.762680Z"
    },
    "papermill": {
     "duration": 34.158958,
     "end_time": "2020-09-04T21:35:55.762828",
     "exception": false,
     "start_time": "2020-09-04T21:35:21.603870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n",
    "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "search_space = [{\"classifier\": [LogisticRegression()],\n",
    "\"classifier__penalty\": ['l1', 'l2'],\n",
    "\"classifier__C\": np.logspace(0, 4, 10)},\n",
    "{\"classifier\": [RandomForestClassifier()],\n",
    "\"classifier__n_estimators\": [10, 100, 1000],\n",
    "\"classifier__max_features\": [1, 2, 3]}]\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n",
    "# View best model\n",
    "best_model.best_estimator_.get_params()[\"classifier\"]\n",
    "\n",
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007796,
     "end_time": "2020-09-04T21:35:55.778691",
     "exception": false,
     "start_time": "2020-09-04T21:35:55.770895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "In the previous two recipes we found the best model by searching over possible\n",
    "hyperparameter values of a learning algorithm. However, what if we are not certain\n",
    "which learning algorithm to use? Recent versions of scikit-learn allow us to include\n",
    "learning algorithms as part of the search space. In our solution we define a search\n",
    "space that includes two learning algorithms: logistic regression and random forest\n",
    "classifier. Each learning algorithm has its own hyperparameters, and we define their\n",
    "candidate values using the format classifier__[hyperparameter name]."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007819,
     "end_time": "2020-09-04T21:35:55.794533",
     "exception": false,
     "start_time": "2020-09-04T21:35:55.786714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Selecting Best Models When Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.007724,
     "end_time": "2020-09-04T21:35:55.810393",
     "exception": false,
     "start_time": "2020-09-04T21:35:55.802669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem\n",
    "You want to include a preprocessing step during model selection.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008141,
     "end_time": "2020-09-04T21:35:55.826470",
     "exception": false,
     "start_time": "2020-09-04T21:35:55.818329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution:\n",
    "Create a pipeline that includes the preprocessing step and any of its parameters:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-04T21:35:55.847190Z",
     "iopub.status.busy": "2020-09-04T21:35:55.846174Z",
     "iopub.status.idle": "2020-09-04T21:35:57.807452Z",
     "shell.execute_reply": "2020-09-04T21:35:57.806857Z"
    },
    "papermill": {
     "duration": 1.972871,
     "end_time": "2020-09-04T21:35:57.807578",
     "exception": false,
     "start_time": "2020-09-04T21:35:55.834707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create a preprocessing object that includes StandardScaler features and PCA\n",
    "preprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"preprocess\", preprocess),\n",
    "(\"classifier\", LogisticRegression())])\n",
    "# Create space of candidate values\n",
    "search_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n",
    "\"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "\"classifier__C\": np.logspace(0, 4, 10)}]\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)\n",
    "\n",
    "# View best model\n",
    "best_model.best_estimator_.get_params()['preprocess__pca__n_components']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008742,
     "end_time": "2020-09-04T21:35:57.825236",
     "exception": false,
     "start_time": "2020-09-04T21:35:57.816494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "have to be careful to properly handle preprocessing when conducting model selection.\n",
    "First, GridSearchCV uses cross-validation to determine which model has the\n",
    "highest performance. However, in cross-validation we are in effect pretending that\n",
    "the fold held out, as the test set is not seen, and thus not part of fitting any preprocessing\n",
    "steps (e.g., scaling or standardization). For this reason, we cannot preprocess\n",
    "the data and then run GridSearchCV. Rather, the preprocessing steps must be a part\n",
    "of the set of actions taken by GridSearchCV. While this might appear complex, the\n",
    "reality is that scikit-learn makes it simple. FeatureUnion allows us to combine multi‐\n",
    "ple preprocessing actions properly. In our solution we use FeatureUnion to combine\n",
    "two preprocessing steps: standardize the feature values (StandardScaler) and Principal\n",
    "Component Analysis (PCA). This object is called preprocess and contains both of\n",
    "our preprocessing steps. We then include preprocess into a pipeline with our learning\n",
    "algorithm. The end result is that this allows us to outsource the proper (and confusing)\n",
    "handling of fitting, transforming, and training the models with combinations\n",
    "of hyperparameters to scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008774,
     "end_time": "2020-09-04T21:35:57.843008",
     "exception": false,
     "start_time": "2020-09-04T21:35:57.834234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Speeding Up Model Selection with Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008642,
     "end_time": "2020-09-04T21:35:57.860453",
     "exception": false,
     "start_time": "2020-09-04T21:35:57.851811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem\n",
    "You need to speed up model selection.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008574,
     "end_time": "2020-09-04T21:35:57.877667",
     "exception": false,
     "start_time": "2020-09-04T21:35:57.869093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution:\n",
    "Use all the cores in your machine by setting n_jobs=-1:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-04T21:35:57.907193Z",
     "iopub.status.busy": "2020-09-04T21:35:57.906157Z",
     "iopub.status.idle": "2020-09-04T21:40:21.688906Z",
     "shell.execute_reply": "2020-09-04T21:40:21.689717Z"
    },
    "papermill": {
     "duration": 263.803343,
     "end_time": "2020-09-04T21:40:21.689957",
     "exception": false,
     "start_time": "2020-09-04T21:35:57.886614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1400 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3400 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 6200 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9800 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:  2.7min finished\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "# Create range of candidate values for C\n",
    "C = np.logspace(0, 4, 1000)\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n",
    "# Create grid search using one core\n",
    "clf = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=1, verbose=1)\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.010415,
     "end_time": "2020-09-04T21:40:21.712709",
     "exception": false,
     "start_time": "2020-09-04T21:40:21.702294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "In the recipes of this chapter, we have kept the number of candidate models small to\n",
    "make the code complete quickly. However, in the real world we will often have many\n",
    "thousands or tens of thousands of models to train. The end result is that it can take\n",
    "many hours to find the best model. To speed up the process, scikit-learn lets us train\n",
    "multiple models simultaneously. Without going into too much technical detail, scikitlearn\n",
    "can simultaneously train models up to the number of cores on the machine.\n",
    "Most modern laptops have four cores, so (assuming you are currently on a laptop) we\n",
    "can potentially train four models at the same time. This will dramatically increase the\n",
    "speed of our model selection process. The parameter n_jobs defines the number of\n",
    "models to train in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008975,
     "end_time": "2020-09-04T21:40:21.731080",
     "exception": false,
     "start_time": "2020-09-04T21:40:21.722105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Speeding Up Model Selection Using Algorithm-Specific Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.009706,
     "end_time": "2020-09-04T21:40:21.749737",
     "exception": false,
     "start_time": "2020-09-04T21:40:21.740031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem\n",
    "You need to speed up model selection.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.012589,
     "end_time": "2020-09-04T21:40:21.774242",
     "exception": false,
     "start_time": "2020-09-04T21:40:21.761653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution:\n",
    "If you are using a select number of learning algorithms, use scikit-learn’s modelspecific\n",
    "cross-validation hyperparameter tuning. For example, LogisticRegres\n",
    "sionCV:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-04T21:40:21.804585Z",
     "iopub.status.busy": "2020-09-04T21:40:21.803700Z",
     "iopub.status.idle": "2020-09-04T21:40:29.585719Z",
     "shell.execute_reply": "2020-09-04T21:40:29.585201Z"
    },
    "papermill": {
     "duration": 7.799972,
     "end_time": "2020-09-04T21:40:29.585828",
     "exception": false,
     "start_time": "2020-09-04T21:40:21.785856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import linear_model, datasets\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create cross-validated logistic regression\n",
    "logit = linear_model.LogisticRegressionCV(Cs=100)\n",
    "# Train model\n",
    "logit.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008701,
     "end_time": "2020-09-04T21:40:29.604079",
     "exception": false,
     "start_time": "2020-09-04T21:40:29.595378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "Sometimes the characteristics of a learning algorithm allow us to search for the best\n",
    "hyperparameters significantly faster than either brute force or randomized model\n",
    "search methods. In scikit-learn, many learning algorithms (e.g., ridge, lasso, and elastic\n",
    "net regression) have an algorithm-specific cross-validation method to take advantage\n",
    "of this. For example, LogisticRegression is used to conduct a standard logistic\n",
    "regression classifier, while LogisticRegressionCV implements an efficient crossvalidated\n",
    "logistic regression classifier that has the ability to identify the optimum\n",
    "value of the hyperparameter C."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00848,
     "end_time": "2020-09-04T21:40:29.621756",
     "exception": false,
     "start_time": "2020-09-04T21:40:29.613276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Evaluating Performance After Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.00912,
     "end_time": "2020-09-04T21:40:29.639740",
     "exception": false,
     "start_time": "2020-09-04T21:40:29.630620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:red\">Problem\n",
    "You want to evaluate the performance of a model found through model selection.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008919,
     "end_time": "2020-09-04T21:40:29.657787",
     "exception": false,
     "start_time": "2020-09-04T21:40:29.648868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:blue\">Solution\n",
    "Use nested cross-validation to avoid biased evaluation:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-04T21:40:29.688563Z",
     "iopub.status.busy": "2020-09-04T21:40:29.686016Z",
     "iopub.status.idle": "2020-09-04T21:40:56.704459Z",
     "shell.execute_reply": "2020-09-04T21:40:56.703833Z"
    },
    "papermill": {
     "duration": 27.037303,
     "end_time": "2020-09-04T21:40:56.704598",
     "exception": false,
     "start_time": "2020-09-04T21:40:29.667295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "# Create range of 20 candidate values for C\n",
    "C = np.logspace(0, 4, 20)\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "# Create grid search\n",
    "\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
    "# Conduct nested cross-validation and outut the average score\n",
    "cross_val_score(gridsearch, features, target).mean()\n",
    "\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n",
    "\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n",
    "scores = cross_val_score(gridsearch, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.009755,
     "end_time": "2020-09-04T21:40:56.724657",
     "exception": false,
     "start_time": "2020-09-04T21:40:56.714902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <b style=\"color:green\">Supported Discussion </b>\n",
    "Nested cross-validation during model selection is a difficult concept for many people\n",
    "to grasp the first time. Remember that in k-fold cross-validation, we train our model\n",
    "on k–1 folds of the data, use this model to make predictions on the remaining fold,\n",
    "and then evaluate our model best on how well our model’s predictions compare to the\n",
    "true values. We then repeat this process k times.\n",
    "In the model selection searches described in this chapter (i.e., GridSearchCV and Ran\n",
    "domizedSearchCV), we used cross-validation to evaluate which hyperparameter values\n",
    "produced the best models. However, a nuanced and generally underappreciated problem\n",
    "arises: since we used the data to select the best hyperparameter values, we cannot\n",
    "use that same data to evaluate the model’s performance. The solution? Wrap the\n",
    "cross-validation used for model search in another cross-validation! In nested crossvalidation,\n",
    "the “inner” cross-validation selects the best model, while the “outer” crossvalidation\n",
    "provides us with an unbiased evaluation of the model’s performance. In\n",
    "our solution, the inner cross-validation is our GridSearchCV object, which we then\n",
    "wrap in an outer cross-validation using cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.009866,
     "end_time": "2020-09-04T21:40:56.744230",
     "exception": false,
     "start_time": "2020-09-04T21:40:56.734364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 352.144245,
   "end_time": "2020-09-04T21:40:57.784515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-04T21:35:05.640270",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
