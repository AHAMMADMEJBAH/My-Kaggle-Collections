{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.compat.v1 as tf\nimport gym\nimport numpy as np\nimport warnings","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.set_random_seed(1)\nnp.random.seed(1)\ntf.disable_eager_execution()\nwarnings.filterwarnings('ignore')","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create HyperParameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"bathc_size = 32\nlearning_rate = 0.01\nepsilon = 0.9\ngamma = 0.9\ntarget_replace_iter = 100\nmemory_capacity = 2000\nmemory_counter = 0\nlearning_step_counter = 0\nenv = gym.make('CartPole-v0')\nenv = env.unwrapped\nn_actions = env.action_space.n\nn_states = env.observation_space.shape[0]\nmemory = np.zeros((memory_capacity, n_states * 2 + 2))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_s = tf.placeholder(tf.float32, [None, n_states])\ntf_a = tf.placeholder(tf.int32, [None, ])\ntf_r = tf.placeholder(tf.float32, [None, ])\ntf_s_ = tf.placeholder(tf.float32, [None, n_states])","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation of Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.variable_scope('q'):\n    l_eval = tf.layers.dense(tf_s, 10, tf.nn.relu, kernel_initializer = tf.random_normal_initializer(0, 0.1))\n    q = tf.layers.dense(l_eval, n_actions, trainable=False)\n","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target Network ( Not to train )"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.variable_scope('q_next'):\n    l_target = tf.layers.dense(tf_s_, 10, tf.nn.relu, trainable= False)\n    q_next = tf.layers.dense(l_target, n_actions, trainable=False)\n    \n    ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_target = tf_r + gamma  * tf.reduce_max(q_next, axis = 1)\na_indices = tf.stack([tf.range(tf.shape(tf_a)[0], dtype = tf.int32), tf_a], axis = 1)\nq_wrt_a = tf.gather_nd(params = q, indices = a_indices)\n\nloss = tf.reduce_mean(tf.squared_difference(q_target, q_wrt_a))\ntrain_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def choose_action(s):\n    s = s[np.newaxis, :]\n    if np.random.uniform() < epsilon:\n        actions_value = sess.run(q, feed_dict = {tf_s:s})\n        action = np.argmax(actions_a_indicesvalue)\n    else:\n        action = np.random.randint(0, n_actions)\n  \n    return action\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def store_transition(s, a, r, s_):\n    global memory_counter\n    transition = np.hstack((s, [a, r], s_))\n    index = memory_counter % memory_capacity\n    memory[index, :] = transition\n    memory_counter+=1\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def learn():\n    # Update the target network\n    global learning_step_counter\n    \n    if learning_step_counter % target_replace_iter == 0:\n        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'q_next')\n        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'q')\n        sess.run([tf.assign(t, e) for t, e, in zip(t_params, e_params)])\n        \n    learning_step_counter += 1\n    \n    sample_index = np.random.choice(memory_capacity, bathc_size)\n    b_memory = memory[sample_index, :]\n    b_s= b_memory[:, n_states]\n    b_a = b_memory[:, n_states].astype(int)\n    b_r = b_memory[:, n_states +1]\n    b_s_ = b_memory[:, -n_states:]\n    \n    sess.run(train_op, {tf_s:b_s, tf_a:b_a, tf_r:b_r,tf_s_:b_s_})\n    ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Collecting Experience ....')\n\nfor i_episode in range(400):\n    s = env.reset()\n    ep_r = 0\n    \n    while True:\n        env.render()\n        a = choice_action(s)\n        \n        s_, r, done, info = env.step(a)\n        \n        x, x_dot, theta, theta_dot = s_\n        \n        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n        r = r1 + r2\n        \n        store_transition(s, a, r, s_)\n        ep_r += r\n        \n        if memory_counter > memory_capacity:\n            learn()\n            if done:\n                print('Ep: ', i_episode, '|Ep_r: ', round(ep_r, 2))\n                \n        if done:\n            break\n            \n        \n    display.clear_output(wait=True)\n    display.display(plt.gcf())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}